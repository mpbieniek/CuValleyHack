{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rl.processors import WhiteningNormalizerProcessor\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.models import Sequential, model_from_json, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.initializers import normal, identity\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential, Model\n",
    "#from keras.engine.training import collect_trainable_weights\n",
    "from keras.layers import Dense, Flatten, Input, merge, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ornstein-Uhlenbeck Process\n",
    "class OU(object):\n",
    "\n",
    "    def function(self, x, mu, theta, sigma):\n",
    "        return theta * (mu - x) + sigma * np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OU = OU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001     #Target Network HyperParameters\n",
    "LRA = 0.0001    #Learning rate for Actor\n",
    "LRC = 0.001     #Lerning rate for Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 4  #Przeplyw powietrza / Zawartosc tlenu / Predkosc Dmuchu / Nadawa pylow\n",
    "                #'001FCx00285_SPPV.PV', '001XXXCALC01.NUM.PV[3]', '001SCx00274_SPPV.PV', '001FCx00241_sppv.pv'\n",
    "state_dim = 22  #parametry wyjsciowe\n",
    "np.random.seed(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = 100000.\n",
    "episode_count = 2000\n",
    "max_steps = 100000\n",
    "reward = 0\n",
    "done = False\n",
    "step = 0\n",
    "epsilon = 1\n",
    "indicator = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = keras.backend.get_session()\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sternik\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "HIDDEN1_UNITS = 300\n",
    "HIDDEN2_UNITS = 600\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    def __init__(self, sess, state_size, action_size, BATCH_SIZE, TAU, LEARNING_RATE):\n",
    "        self.sess = sess\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.TAU = TAU\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "\n",
    "        K.set_session(sess)\n",
    "\n",
    "        #Now create the model\n",
    "        self.model , self.weights, self.state = self.create_actor_network(state_size, action_size)   \n",
    "        self.target_model, self.target_weights, self.target_state = self.create_actor_network(state_size, action_size) \n",
    "        self.action_gradient = tf.compat.v1.placeholder(tf.float32,[None, action_size])\n",
    "        self.params_grad = tf.gradients(self.model.output, self.weights, -self.action_gradient)\n",
    "        grads = zip(self.params_grad, self.weights)\n",
    "        self.optimize = tf.optimizers.Adam(LEARNING_RATE).apply_gradients(grads)\n",
    "        self.sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "    def train(self, states, action_grads):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.state: states,\n",
    "            self.action_gradient: action_grads\n",
    "        })\n",
    "\n",
    "    def target_train(self):\n",
    "        actor_weights = self.model.get_weights()\n",
    "        actor_target_weights = self.target_model.get_weights()\n",
    "        for i in xrange(len(actor_weights)):\n",
    "            actor_target_weights[i] = self.TAU * actor_weights[i] + (1 - self.TAU)* actor_target_weights[i]\n",
    "        self.target_model.set_weights(actor_target_weights)\n",
    "\n",
    "    def create_actor_network(self, state_size,action_dim):\n",
    "        S = Input(shape=[state_size])   \n",
    "        h0 = Dense(HIDDEN1_UNITS, activation='relu')(S)\n",
    "        h1 = Dense(HIDDEN2_UNITS, activation='relu')(h0)\n",
    "        Przeplyw = Dense(1,activation='sigmoid')(h1)  \n",
    "        Predkosc = Dense(1,activation='sigmoid')(h1)   \n",
    "        Tlen = Dense(1,activation='sigmoid')(h1) \n",
    "        Pyly = Dense(1,activation='relu')(h1) \n",
    "        V = merge.concatenate([Przeplyw,Predkosc,Tlen,Pyly], axis=-1)        \n",
    "        model = Model(inputs=S,outputs=V)\n",
    "        return model, model.trainable_weights, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    def __init__(self, sess, state_size, action_size, BATCH_SIZE, TAU, LEARNING_RATE):\n",
    "        self.sess = sess\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.TAU = TAU\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        K.set_session(sess)\n",
    "\n",
    "        #Now create the model\n",
    "        self.model, self.action, self.state = self.create_critic_network(state_size, action_size)  \n",
    "        self.target_model, self.target_action, self.target_state = self.create_critic_network(state_size, action_size)  \n",
    "        self.action_grads = tf.gradients(self.model.output, self.action)  #GRADIENTS for policy update\n",
    "        self.sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "    def gradients(self, states, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.state: states,\n",
    "            self.action: actions\n",
    "        })[0]\n",
    "\n",
    "    def target_train(self):\n",
    "        critic_weights = self.model.get_weights()\n",
    "        critic_target_weights = self.target_model.get_weights()\n",
    "        for i in xrange(len(critic_weights)):\n",
    "            critic_target_weights[i] = self.TAU * critic_weights[i] + (1 - self.TAU)* critic_target_weights[i]\n",
    "        self.target_model.set_weights(critic_target_weights)\n",
    "\n",
    "    def create_critic_network(self, state_size,action_dim):\n",
    "        S = Input(shape=[state_size])  \n",
    "        A = Input(shape=[action_dim],name='action2')   \n",
    "        w1 = Dense(HIDDEN1_UNITS, activation='relu')(S)\n",
    "        a1 = Dense(HIDDEN2_UNITS, activation='linear')(A) \n",
    "        h1 = Dense(HIDDEN2_UNITS, activation='linear')(w1)\n",
    "        h2 = merge.add([h1,a1])    \n",
    "        h3 = Dense(HIDDEN2_UNITS, activation='relu')(h2)\n",
    "        V = Dense(action_dim,activation='linear')(h3)   \n",
    "        model = Model(inputs=[S,A],outputs=V)\n",
    "        #adam = Adam(lr=self.LEARNING_RATE)\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        return model, A, S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "Now we build the model\n",
      "WARNING:tensorflow:From /home/mateusz/.local/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "critic = CriticNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        if self.num_experiences < batch_size:\n",
    "            return random.sample(self.buffer, self.num_experiences)\n",
    "        else:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, new_state):\n",
    "        experience = (state, action, reward, new_state)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff = ReplayBuffer(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_seq(data_x, data_y, steps):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(data_x) - steps):\n",
    "        end = i + steps\n",
    "        seq_x = data_x[i:end, :]\n",
    "        seq_y = data_y[end, :]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../../zadanie2_7z/hmg/joined/data_2021-04-19.csv').drop(columns = ['Unnamed: 0'])\n",
    "data_y = data.drop(columns=['001FCx00285_SPPV.PV', '001XXXCALC01.NUM.PV[3]', '001SCx00274_SPPV.PV', '001FCx00241_sppv.pv']).to_numpy()\n",
    "data_x = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Furnance:\n",
    "    terminal_judge_start = 100  # If after 100 timestep still no progress, terminated\n",
    "    termination_limit_progress = 5  # [km/h], episode terminates if car is running slower than this limit\n",
    "    default_speed = 50\n",
    "\n",
    "    initial_reset = True\n",
    "\n",
    "    def __init__(self, throttle=False, gear_change=False):\n",
    "        \n",
    "        self.time_step = 0\n",
    "        data = pd.read_csv('../../../zadanie2_7z/hmg/joined/data_2021-04-19.csv').drop(columns = ['Unnamed: 0'])\n",
    "        data_y = data.drop(columns=['001FCx00285_SPPV.PV', '001XXXCALC01.NUM.PV[3]', '001SCx00274_SPPV.PV', '001FCx00241_sppv.pv']).to_numpy()\n",
    "        data_x = data.to_numpy()\n",
    "        x, y = split_seq(data_x, data_y, steps = 5)\n",
    "        self.x = x[0].reshape(1,5,26)\n",
    "        self.y = y[0]\n",
    "        self.model = None\n",
    "        self.model = keras.models.load_model('../../../zadanie2_7z/hmg/results/RL/model_last4.h5')\n",
    "        print(\"model loaded\")\n",
    "\n",
    "    def model_pred(self, input_x = None):\n",
    "        \n",
    "        if input_x is None:\n",
    "            input_x = self.initial_state\n",
    "        y_pred = self.model.predict(input_x, verbose=0)\n",
    "        \n",
    "        return(y_pred)\n",
    "        \n",
    "    def step(self, x):\n",
    "\n",
    "        y = self.model_pred(x)\n",
    "        \n",
    "        heat_loss = abs(y[0][0] - self.y[0]) * -1\n",
    "        settings = x[0, 4, :4]\n",
    "        \n",
    "        reward = heat_loss\n",
    "        \n",
    "        if settings[0] < 1900 or settings[0] > 3500:\n",
    "            reward -= 4 # przeplyw \n",
    "        if settings[1] < 65 or settings[1] > 81:\n",
    "            reward -= 4 # tlen\n",
    "        if settings[2] < 40 or settings[2] > 70:\n",
    "            reward -= 4 # predkosc\n",
    "        if settings[3] < 13:\n",
    "            reward -= 6 # predkosc\n",
    "        elif settings[3] < 22:\n",
    "            reward -= 1\n",
    "        elif settings[3] > 27:\n",
    "            reward -= 6\n",
    "        \n",
    "        self.y = y\n",
    "\n",
    "        self.time_step += 1\n",
    "\n",
    "        return self.y, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "Now we build the model\n",
      "model loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e029719ad3c44bf49a8fd675d408d4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0 Replay Buffer 0\n",
      "\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Could not find variable training_2/Adam/iter. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training_2/Adam/iter/N10tensorflow3VarE does not exist.\n\t [[{{node training_2/Adam/ReadVariableOp}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a88f4eea9782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_indicator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0ma_for_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_for_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4017\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4019\u001b[0;31m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[1;32m   4020\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[1;32m   4021\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Could not find variable training_2/Adam/iter. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status=Not found: Resource localhost/training_2/Adam/iter/N10tensorflow3VarE does not exist.\n\t [[{{node training_2/Adam/ReadVariableOp}}]]"
     ]
    }
   ],
   "source": [
    "actor = ActorNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRA)\n",
    "critic = CriticNetwork(sess, state_dim, action_dim, BATCH_SIZE, TAU, LRC)\n",
    "buff = ReplayBuffer(BUFFER_SIZE)\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "K.set_session(sess)\n",
    "train_indicator = 1\n",
    "\n",
    "ob = Furnance()\n",
    "#initial y\n",
    "s_t = ob.y\n",
    "\n",
    "for i in tqdm(range(episode_count)):\n",
    "\n",
    "        print(\"Episode : \" + str(i) + \" Replay Buffer \" + str(buff.count()))\n",
    "     \n",
    "        total_reward = 0.\n",
    "        for j in range(max_steps):\n",
    "            loss = 0 \n",
    "            epsilon -= 1.0 / EXPLORE\n",
    "            a_t = np.zeros([1, action_dim])\n",
    "            noise_t = np.zeros([1, action_dim])\n",
    "            \n",
    "            a_t_original = actor.model.predict(s_t.reshape(1, s_t.shape[0]))\n",
    "            noise_t[0][0] = train_indicator * max(epsilon, 0) * OU.function(a_t_original[0][0],  2700 , 60, 80)\n",
    "            noise_t[0][1] = train_indicator * max(epsilon, 0) * OU.function(a_t_original[0][1],  70 , 0.013, 0.013)\n",
    "            noise_t[0][2] = train_indicator * max(epsilon, 0) * OU.function(a_t_original[0][2], 50 , 2, 2)\n",
    "            noise_t[0][3] = train_indicator * max(epsilon, 0) * OU.function(a_t_original[0][2], 25 , 12/300, 12)\n",
    "\n",
    "            a_t[0][0] = a_t_original[0][0] + noise_t[0][0]\n",
    "            a_t[0][1] = a_t_original[0][1] + noise_t[0][1]\n",
    "            a_t[0][2] = a_t_original[0][2] + noise_t[0][2]\n",
    "            a_t[0][3] = a_t_original[0][3] + noise_t[0][3]\n",
    "\n",
    "            nx = ob.x[:, 1:, :]\n",
    "            new = np.concatenate((a_t[0], s_t)).reshape(1, 1, 26)\n",
    "            nx = np.concatenate((nx, new), axis = 1)\n",
    "            \n",
    "            #r_t = reward\n",
    "            s_t1, r_t = ob.step(nx)\n",
    "        \n",
    "            buff.add(s_t, a_t[0], r_t, s_t1)      #Add replay buffer\n",
    "            \n",
    "            #Do the batch update\n",
    "            batch = buff.getBatch(BATCH_SIZE)\n",
    "            states = np.asarray([e[0] for e in batch])\n",
    "            actions = np.asarray([e[1] for e in batch])\n",
    "            rewards = np.asarray([e[2] for e in batch])\n",
    "            new_states = np.asarray([e[3] for e in batch])\n",
    "            y_t = np.asarray([e[1] for e in batch])\n",
    "\n",
    "            target_q_values = critic.target_model.predict([new_states[0], actor.target_model.predict(new_states[0])])  \n",
    "           \n",
    "            for k in range(len(batch)):\n",
    "                y_t[k] = rewards[k] + GAMMA*target_q_values[k]\n",
    "       \n",
    "            if (train_indicator):\n",
    "                loss += critic.model.train_on_batch([states,actions], y_t) \n",
    "                a_for_grad = actor.model.predict(states)\n",
    "                grads = critic.gradients(states, a_for_grad)\n",
    "                actor.train(states, grads)\n",
    "                actor.target_train()\n",
    "                critic.target_train()\n",
    "\n",
    "            total_reward += r_t\n",
    "            s_t = s_t1\n",
    "        \n",
    "            print(\"Episode\", i, \"Step\", step, \"Action\", a_t, \"Reward\", r_t, \"Loss\", loss)\n",
    "        \n",
    "            step += 1\n",
    "\n",
    "        if np.mod(i, 3) == 0:\n",
    "            if (train_indicator):\n",
    "                print(\"Now we save model\")\n",
    "                actor.model.save(\"../../../zadanie2_7z/hmg/results/RL/actormodel.h5\", overwrite=True)\n",
    "                critic.model.save(\"../../../zadanie2_7z/hmg/results/RL/criticmodel.h5\", overwrite=True)\n",
    "\n",
    "        print(\"TOTAL REWARD @ \" + str(i) +\"-th Episode  : Reward \" + str(total_reward))\n",
    "        print(\"Total Step: \" + str(step))\n",
    "        print(\"\")\n",
    "\n",
    "print(\"Finish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
